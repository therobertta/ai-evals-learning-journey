# Complete AI Evals Learning Journey Setup Guide

## üéØ Overview
This file contains everything needed to set up your systematic AI evaluation learning journey in your forked recipe-chatbot repository.

## üìã Quick Setup Checklist
- [ ] Copy all learning framework files to your fork
- [ ] Create GitHub project board
- [ ] Create GitHub issues using the templates below
- [ ] Start with Epic 0.1

---

## üìÅ Files to Copy to Your Fork

### Core Learning Framework Files
Copy these files from this directory to your forked repo:

1. **AI_EVALS_LEARNING_FRAMEWORK.md** - Complete learning methodology
2. **COMPLETE_HOMEWORK_GITHUB_PLAN.md** - Detailed issue breakdown
3. **CLAUDE_CODE_BEST_PRACTICES.md** - Best practices guide
4. **.github/ISSUE_TEMPLATE/learning-issue.md** - Issue template
5. **.github/pull_request_template.md** - PR template

---

## üèóÔ∏è GitHub Project Board Setup

### Create Project Board
1. Go to your repository: https://github.com/therobertta/recipe-chatbot
2. Click **Projects** ‚Üí **New Project**
3. Choose **Board** layout
4. Name: "AI Evals Learning Journey"

### Board Columns
Create these columns in order:
1. üìã **Backlog** - Planned work items
2. üî¨ **In Analysis** - Research and planning phase
3. üíª **In Development** - Active coding/implementation
4. üìù **In Documentation** - Writing learnings and reports
5. üîç **In Review** - Self-review and validation
6. ‚úÖ **Done** - Completed with learnings documented

---

## üé´ GitHub Issues to Create

### Epic 0: Foundation & Retrospective Analysis

#### Issue 0.1: Setup Learning Repository and Documentation Framework
**Title**: `Epic 0.1 - Setup Learning Repository and Documentation Framework`
**Labels**: `epic-0`, `setup`, `documentation`, `learning-foundation`
**Body**:
```markdown
## Learning Objective
Establish systematic documentation practices and create reusable evaluation frameworks for the complete AI evals learning journey.

## Connection to AI Evals Framework
This provides the foundation for documenting learnings throughout the Analyze-Measure-Improve lifecycle.

## Tasks
- [ ] Create comprehensive documentation structure
- [ ] Set up learning templates and best practices
- [ ] Establish GitHub workflow patterns with proper branching
- [ ] Create issue and PR templates for systematic learning capture
- [ ] Set up project board with proper workflow columns

## Learning Goals
- **Primary Skill**: Systematic learning documentation and knowledge management
- **Secondary Skills**: GitHub project management and workflow design
- **Connection**: Enables systematic capture of evaluation methodology learnings

## Deliverables
- [ ] **Code**: Documentation templates and workflow configurations
- [ ] **Documentation**: Learning framework and best practices guide
- [ ] **Analysis**: Documentation strategy and methodology

## Definition of Done
- [ ] All documentation templates created and tested
- [ ] GitHub project board configured with proper workflow
- [ ] Issue and PR templates validated
- [ ] Learning capture methodology documented
- [ ] PR created with comprehensive setup

## Learning Notes
[Space for capturing insights about systematic learning documentation]

## Claude Code Best Practices Applied
- [ ] Systematic file organization and navigation
- [ ] Concurrent tool usage for efficient setup
- [ ] Proper documentation structure
```

#### Issue 0.2: Retrospective Analysis of HW1 Implementation
**Title**: `Epic 0.2 - Retrospective Analysis of HW1 Implementation`
**Labels**: `epic-0`, `hw1`, `retrospective`, `prompt-engineering`
**Body**:
```markdown
## Learning Objective
Understand prompt engineering fundamentals and analyze initial system design decisions from HW1.

## Connection to AI Evals Framework
This retrospective analysis connects to the 'Analyze' phase - understanding what was built and why.

## Tasks
- [ ] Document HW1 system prompt design process and decisions
- [ ] Analyze query dataset expansion strategy and effectiveness
- [ ] Extract key learnings from initial prompt engineering
- [ ] Review bulk test results and identify patterns
- [ ] Document what worked well vs what could be improved

## Learning Goals
- **Primary Skill**: Retrospective analysis and prompt engineering evaluation
- **Secondary Skills**: System design documentation and decision analysis
- **Connection**: Provides foundation for understanding prompt specification challenges

## Deliverables
- [ ] **Code**: Analysis scripts for reviewing HW1 implementation
- [ ] **Documentation**: HW1 retrospective report with key learnings
- [ ] **Analysis**: Prompt effectiveness analysis and improvement recommendations

## Definition of Done
- [ ] Complete analysis of HW1 system prompt documented
- [ ] Query dataset strategy analyzed and documented
- [ ] Key learnings extracted and synthesized
- [ ] Improvement recommendations for future prompt engineering
- [ ] PR created with comprehensive retrospective analysis

## Learning Notes
[Space for capturing insights about prompt engineering and system design]

## Claude Code Best Practices Applied
- [ ] Systematic file reading and analysis
- [ ] Structured documentation approach
- [ ] Comprehensive codebase examination
```

### Epic 1: HW2 - Error Analysis Mastery

#### Issue 1.1: HW2 Environment Setup and Data Exploration
**Title**: `Epic 1.1 - HW2 Environment Setup and Data Exploration`
**Labels**: `epic-1`, `hw2`, `setup`, `data-exploration`
**Body**:
```markdown
## Learning Objective
Understand trace data structure and evaluation workflows while mastering Claude Code for systematic analysis.

## Connection to AI Evals Framework
This is the foundation for the 'Analyze' phase - understanding your data before systematic error analysis.

## Tasks
- [ ] Examine existing data files (results_20250518_215844.csv, hw2_bot_traces_20250613.csv)
- [ ] Understand trace data structure and format
- [ ] Set up analysis environment and tools
- [ ] Create data exploration scripts using Claude Code best practices
- [ ] Document data quality and characteristics

## Learning Goals
- **Primary Skill**: Systematic data exploration and trace analysis
- **Secondary Skills**: Claude Code proficiency for file analysis
- **Connection**: Essential foundation for open and axial coding in error analysis

## Deliverables
- [ ] **Code**: Data exploration scripts and analysis tools
- [ ] **Documentation**: Data structure documentation and quality assessment
- [ ] **Analysis**: Initial data exploration findings and insights

## Definition of Done
- [ ] All existing data files analyzed and documented
- [ ] Data exploration scripts created and tested
- [ ] Data quality assessment completed
- [ ] Analysis environment properly configured
- [ ] PR created with comprehensive data exploration

## Learning Notes
[Space for capturing insights about trace data structure and analysis approaches]

## Claude Code Best Practices Applied
- [ ] Concurrent file reading for efficient analysis
- [ ] Systematic data exploration methodology
- [ ] Proper error handling for data quality issues
```

#### Issue 1.2: Master Error Analysis Theory and Open Coding
**Title**: `Epic 1.2 - Master Error Analysis Theory and Open Coding`
**Labels**: `epic-1`, `hw2`, `theory`, `open-coding`, `methodology`
**Body**:
```markdown
## Learning Objective
Understand open coding vs axial coding methodologies and learn when to apply each technique.

## Connection to AI Evals Framework
This is core to the 'Analyze' phase - the systematic methodology for understanding failure modes.

## Tasks
- [ ] Study course materials on error analysis (sections 3.2-3.4)
- [ ] Create methodology guides for open coding process
- [ ] Practice open coding on sample traces
- [ ] Document when to use open vs axial coding
- [ ] Create reusable templates for coding process

## Learning Goals
- **Primary Skill**: Open coding methodology and systematic error analysis
- **Secondary Skills**: Academic methodology application and documentation
- **Connection**: Essential foundation for systematic failure mode identification

## Deliverables
- [ ] **Code**: Open coding analysis templates and tools
- [ ] **Documentation**: Complete error analysis methodology guide
- [ ] **Analysis**: Practice coding results on sample data

## Definition of Done
- [ ] Open coding methodology fully documented
- [ ] Practice coding completed on sample traces
- [ ] Methodology guide created for future reference
- [ ] Templates created for systematic coding process
- [ ] PR created with complete theoretical foundation

## Learning Notes
[Space for capturing insights about systematic error analysis methodology]

## Claude Code Best Practices Applied
- [ ] Systematic documentation structure
- [ ] Methodical approach to analysis
- [ ] Reusable template creation
```

#### Issue 1.3: Define Recipe Bot Test Dimensions
**Title**: `Epic 1.3 - Define Recipe Bot Test Dimensions`
**Labels**: `epic-1`, `hw2`, `test-design`, `dimensions`
**Body**:
```markdown
## Learning Objective
Learn systematic test case design and understand critical input variable identification.

## Connection to AI Evals Framework
This connects to the 'Analyze' phase - systematically understanding the input space for comprehensive evaluation.

## Tasks
- [ ] Identify 3-4 key dimensions (cuisine_type, dietary_restriction, meal_type, complexity)
- [ ] Define 3+ values per dimension with clear rationale
- [ ] Validate dimensions against real user interaction patterns
- [ ] Document dimension selection methodology
- [ ] Create systematic test coverage framework

## Learning Goals
- **Primary Skill**: Systematic test case design and input space analysis
- **Secondary Skills**: User interaction pattern analysis and validation
- **Connection**: Essential for comprehensive failure mode discovery

## Deliverables
- [ ] **Code**: Dimension validation and analysis scripts
- [ ] **Documentation**: Dimensions specification with rationale
- [ ] **Analysis**: Test coverage analysis and validation report

## Definition of Done
- [ ] 3-4 key dimensions identified and documented
- [ ] Values per dimension defined with clear rationale
- [ ] Validation against user patterns completed
- [ ] Test coverage framework documented
- [ ] PR created with systematic dimension analysis

## Learning Notes
[Space for capturing insights about systematic test design]

## Claude Code Best Practices Applied
- [ ] Systematic analysis methodology
- [ ] Comprehensive validation approach
- [ ] Structured documentation
```

#### Issue 1.4: Generate Synthetic Query Combinations
**Title**: `Epic 1.4 - Generate Synthetic Query Combinations`
**Labels**: `epic-1`, `hw2`, `synthetic-data`, `query-generation`
**Body**:
```markdown
## Learning Objective
Master prompt engineering for systematic data generation and understand test coverage principles.

## Connection to AI Evals Framework
Critical for the 'Analyze' phase - generating comprehensive test cases to understand system behavior.

## Tasks
- [ ] Create LLM prompts for tuple generation
- [ ] Generate 15-20 unique combinations of dimension values
- [ ] Validate diversity and realism of combinations
- [ ] Document generation methodology and prompts
- [ ] Analyze coverage of test space

## Learning Goals
- **Primary Skill**: Systematic test data generation and prompt engineering
- **Secondary Skills**: Test coverage analysis and validation
- **Connection**: Foundation for comprehensive system evaluation

## Deliverables
- [ ] **Code**: Generation scripts and prompt templates
- [ ] **Documentation**: Generation methodology and validation approach
- [ ] **Analysis**: Coverage analysis and quality assessment

## Definition of Done
- [ ] 15-20 unique, validated combinations generated
- [ ] Generation prompts documented and reusable
- [ ] Diversity and coverage analysis completed
- [ ] Quality validation methodology established
- [ ] PR created with systematic generation approach

## Learning Notes
[Space for capturing insights about systematic data generation]

## Claude Code Best Practices Applied
- [ ] Efficient prompt engineering workflows
- [ ] Systematic validation processes
- [ ] Comprehensive documentation
```

#### Issue 1.5: Convert Tuples to Natural Language Queries
**Title**: `Epic 1.5 - Convert Tuples to Natural Language Queries`
**Labels**: `epic-1`, `hw2`, `natural-language`, `query-conversion`
**Body**:
```markdown
## Learning Objective
Learn realistic test case creation and understand user interaction authenticity.

## Connection to AI Evals Framework
Essential for 'Analyze' phase - creating realistic user inputs that reveal true system behavior.

## Tasks
- [ ] Design LLM prompts for natural language generation
- [ ] Generate 5-7 realistic user queries from tuples
- [ ] Manual review and refinement for authenticity
- [ ] Document conversion methodology
- [ ] Validate against real user patterns

## Learning Goals
- **Primary Skill**: Natural language generation and user interaction modeling
- **Secondary Skills**: Authenticity validation and refinement processes
- **Connection**: Critical for realistic system evaluation

## Deliverables
- [ ] **Code**: Conversion scripts and validation tools
- [ ] **Documentation**: Conversion methodology and quality criteria
- [ ] **Analysis**: Authenticity assessment and refinement results

## Definition of Done
- [ ] 5-7 high-quality natural language queries created
- [ ] Conversion methodology documented and validated
- [ ] Manual review and refinement completed
- [ ] Quality criteria established and applied
- [ ] PR created with conversion framework

## Learning Notes
[Space for capturing insights about natural language generation]

## Claude Code Best Practices Applied
- [ ] Systematic conversion processes
- [ ] Quality validation workflows
- [ ] User-centric design approach
```

---

## üöÄ Getting Started Commands

### After setting up your new repository:

```bash
# Clone your fork
git clone https://github.com/therobertta/recipe-chatbot.git
cd recipe-chatbot

# Copy the learning framework files (you'll do this manually)
# Then commit them
git add .
git commit -m "Add AI evals learning framework

- Complete learning methodology with Analyze-Measure-Improve lifecycle
- Systematic GitHub workflow with 40+ planned issues
- Issue and PR templates for learning capture
- Claude Code best practices for AI-native development

ü§ñ Generated with Claude Code (https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>"

git push origin main

# Start your first branch
git checkout -b feature/epic-0-1-setup

# Begin working on Epic 0.1
```

### File Copy Checklist
Copy these files from ai-evals-learning to your fork:
- [ ] AI_EVALS_LEARNING_FRAMEWORK.md
- [ ] COMPLETE_HOMEWORK_GITHUB_PLAN.md  
- [ ] CLAUDE_CODE_BEST_PRACTICES.md
- [ ] .github/ISSUE_TEMPLATE/learning-issue.md
- [ ] .github/pull_request_template.md

---

## üéØ Success Metrics

### Epic 0 Success Criteria
- [ ] Complete documentation framework established
- [ ] GitHub project board configured with proper workflow
- [ ] All issues created and properly labeled
- [ ] First branch created and ready for development
- [ ] Learning capture methodology validated

### Overall Learning Journey Success
- [ ] **Methodological Mastery**: Complete understanding of Analyze-Measure-Improve lifecycle
- [ ] **Technical Proficiency**: Advanced Claude Code usage for AI evaluation
- [ ] **Production Readiness**: Ability to implement evaluation systems in production
- [ ] **Epistemic Bridge**: Clear path from traditional to epistemic evaluation
- [ ] **Documentation Excellence**: Comprehensive learning artifacts for future reference

---

## üîÑ Next Steps After Setup

1. **Epic 0.1**: Set up learning infrastructure
2. **Epic 0.2**: Analyze HW1 retrospectively
3. **Epic 1.x**: Master HW2 error analysis
4. **Epic 2.x**: Implement LLM-as-Judge (HW3)
5. **Epic 3.x**: Build RAG evaluation system (HW4)
6. **Epic 4.x**: Analyze agent failures (HW5)
7. **Epic 5.x**: Bridge to epistemic evaluation

**Your systematic AI evaluation learning journey starts now!**